{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e010e03f",
   "metadata": {},
   "source": [
    "We begin with a simple plot to give us a starting point and validate that our simulation is giving sensible results. We would expect that significantly below the critical value, all of the percolation clusters in our simulation would terminate, whilst significantly above the critical value there would be one single cluster which dominates, and does not terminate. A simple plot of log cluster size against log number of clusters over a range of probabilities shows this clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da42f66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def plot_size_num_prob_3d(directory, bins=False):\n",
    "  params = []\n",
    "  x1data = [] # terminated clusters\n",
    "  x2data = [] # non-terminated clusters\n",
    "  ydata = []\n",
    "  zdata = []\n",
    "\n",
    "  for filename in sorted(os.listdir(directory)):\n",
    "    path = os.path.join(directory, filename)\n",
    "\n",
    "    with open(path) as fp:\n",
    "      for i, line in enumerate(fp):\n",
    "          if i == 1:\n",
    "            params = line.split(',')\n",
    "            break\n",
    "\n",
    "    data = np.genfromtxt(path, delimiter=',', skip_header=4)\n",
    "\n",
    "    if bins == True:\n",
    "      x1data += list(np.log2(data[:,1] / int(params[3])))\n",
    "      x2data += list(np.log2(data[:,2] / int(params[3])))\n",
    "      zdata += list(data[:,0])\n",
    "    else:\n",
    "      x1data += list(np.log(data[:,1]))\n",
    "      x2data += list(np.log(data[:,2]))\n",
    "      zdata += list(np.log(data[:,0]))\n",
    "    ydata += [float(params[0]) for _ in data[:,0]]\n",
    "\n",
    "  fig = plt.figure()\n",
    "  ax = fig.add_subplot(projection='3d')\n",
    "  ax.scatter(x1data, ydata, zdata)\n",
    "  ax.scatter(x2data, ydata, zdata)\n",
    "  fig.show()\n",
    "   \n",
    "\n",
    "plot_size_num_prob_3d(\"data/p_24_26\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991f9e3e",
   "metadata": {},
   "source": [
    "Let $n_s(p)$ be the average number of clusters per lattice point of size $s$ for a given probability $p$. It is known that at the critical threshold $p_c$,\n",
    "\n",
    "$n_s(p_c) = s^{-\\tau}(c_0+c_1s^{-\\Omega}+...)$\n",
    "\n",
    "where $\\tau$ is the _fisher exponent_ and $\\Omega$ accounts for the leading errors due to the finite size of our lattices.\n",
    "\n",
    "Taking logs, pulling out a constant factor and taylor expanding, we obtain\n",
    "\n",
    "$\\log(n_s) = -\\tau \\log(s) + \\log(1+a_1s^{-\\Omega} + O(s^{-2\\Omega})) + a_2= -\\tau \\log(s) + a_1s^{-\\Omega} + a_2 + O(s^{-2\\Omega})$\n",
    "\n",
    "Thus (aside from the effects of $\\Omega$) we expect the critical value to occur when we have a log-linear relationship between $n_s$ and $s$. This aligns well with our initial sanity-check plot.\n",
    "\n",
    "Thus to obtain accurate estimates for $p_c$ (and $\\tau$) it remains to generate as much data as possible on the relationship between $n_s$ and $s$, whilst minimising the effects of finite lattice sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835c2945",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_size_num_prob_3d(\"data/p_244\", bins=True)\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def rhs(log2_size, tau, omega, a1, a2):\n",
    "  return -tau * log2_size + a1 * 2**(-log2_size * omega) + a2\n",
    "\n",
    "directory = \"data/p_244\"\n",
    "params = []\n",
    "\n",
    "fig, axs = plt.subplots(8)\n",
    "fig.set_figheight(20)\n",
    "\n",
    "for idx, filename in enumerate(sorted(os.listdir(directory))):\n",
    "  path = os.path.join(directory, filename)\n",
    "\n",
    "  with open(path) as fp:\n",
    "    for i, line in enumerate(fp):\n",
    "      if i == 1:\n",
    "        params = line.split(',')\n",
    "        break\n",
    "\n",
    "  data = np.genfromtxt(path, delimiter=',', skip_header=4)\n",
    "\n",
    "  size_data = [d for i, d in enumerate(data[:, 0]) if data[i, 1] + data[i, 2] != 0]\n",
    "  number_data = [np.log2(d + data[i, 2]) for i, d in enumerate(data[:, 1]) if d + data[i, 2] != 0]\n",
    "  \n",
    "  popt, pcov = curve_fit(rhs, size_data, number_data, bounds=(-50,100))\n",
    "\n",
    "  print(popt)\n",
    "  print(np.sqrt(np.diag(pcov)))\n",
    "  axs[idx].scatter(size_data, number_data)\n",
    "\n",
    "  pred_num_data = [rhs(s, *popt) for s in size_data]\n",
    "  axs[idx].plot(size_data, pred_num_data)\n",
    "\n",
    "  axs[idx].set_xlabel(\"log size\")\n",
    "  axs[idx].set_title(params[0])\n",
    "  axs[idx].set_ylabel(\"log num_clusters\")\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
