{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e010e03f",
   "metadata": {},
   "source": [
    "We begin with a simple plot to give us a starting point and validate that our simulation is giving sensible results. We would expect that significantly below the critical value, all of the percolation clusters in our simulation would terminate, whilst significantly above the critical value there would be one single cluster which dominates, and does not terminate. A simple plot of log cluster size against log number of clusters over a range of probabilities shows this clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da42f66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def plot_size_num_prob_3d(directory, bins=False):\n",
    "  params = []\n",
    "  x1data = [] # terminated clusters\n",
    "  x2data = [] # non-terminated clusters\n",
    "  ydata = []\n",
    "  zdata = []\n",
    "\n",
    "  for filename in sorted(os.listdir(directory)):\n",
    "    path = os.path.join(directory, filename)\n",
    "\n",
    "    with open(path) as fp:\n",
    "      for i, line in enumerate(fp):\n",
    "          if i == 1:\n",
    "            params = line.split(',')\n",
    "            break\n",
    "\n",
    "    data = np.genfromtxt(path, delimiter=',', skip_header=4)\n",
    "\n",
    "    if bins == True:\n",
    "      x1data += list(np.log2(data[:,1] / int(params[3])))\n",
    "      x2data += list(np.log2(data[:,2] / int(params[3])))\n",
    "      zdata += list(data[:,0])\n",
    "    else:\n",
    "      x1data += list(np.log(data[:,1]))\n",
    "      x2data += list(np.log(data[:,2]))\n",
    "      zdata += list(np.log(data[:,0]))\n",
    "    ydata += [float(params[0]) for _ in data[:,0]]\n",
    "\n",
    "  fig = plt.figure()\n",
    "  ax = fig.add_subplot(projection='3d')\n",
    "  ax.scatter(x1data, ydata, zdata)\n",
    "  ax.scatter(x2data, ydata, zdata)\n",
    "  fig.show()\n",
    "   \n",
    "\n",
    "plot_size_num_prob_3d(\"data/p_24_26\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991f9e3e",
   "metadata": {},
   "source": [
    "Let $n_s(p)$ be the average number of clusters per lattice point of size $s$ for a given probability $p$. It is known that at the critical threshold $p_c$,\n",
    "\n",
    "$n_s(p_c) = s^{-\\tau}(c_0+c_1s^{-\\Omega}+...)$\n",
    "\n",
    "where $\\tau$ is the _fisher exponent_ and $\\Omega$ accounts for the leading errors due to the finite size of our lattices.\n",
    "\n",
    "Taking logs, pulling out a constant factor and taylor expanding, we obtain\n",
    "\n",
    "$\\log(n_s) = -\\tau \\log(s) + \\log(1+a_1s^{-\\Omega} + O(s^{-2\\Omega})) + a_2= -\\tau \\log(s) + a_1s^{-\\Omega} + a_2 + O(s^{-2\\Omega})$\n",
    "\n",
    "Thus (aside from the effects of $\\Omega$) we expect the critical value to occur when we have a log-linear relationship between $n_s$ and $s$. This aligns well with our initial sanity-check plot.\n",
    "\n",
    "It will be convenient for our simulations to sample individual points from our lattice, rather than counting clusters. So we instead consider the probability $P(s)$ that a given point lives in a cluster of size $s$, which is simply $sn_s$.\n",
    "\n",
    "One obstacle we must overcome if we wish to make accurate predictions is the fact that our simulations must always be finite. In order to remove boundary effects, it is convenient to consider the cumulative distribution $Q(s) = \\sum_{t=s}^{\\infty}P(t)$. This way, we can simply collect up all the terms affected by the boundary into the final bin. Approximating this as an integral, we obtain\n",
    "\n",
    "$Q(s) = \\int_{s}^{\\infty} t^{1-\\tau}(c_0+c_1t^{-\\Omega}+O(...))dt + O(...) = s^{2-\\tau}(c+0+c_1s^{-\\Omega}) + O(...)$\n",
    "\n",
    "and so\n",
    "\n",
    "$\\log Q(s) = (2-\\tau) \\log(s) + a_1s^{-\\Omega} + a_2 + O(...)$\n",
    "\n",
    "Thus to obtain accurate estimates for $p_c$ (and $\\tau$) it remains to generate as much data as possible on the relationship between $n_s$ and $s$, whilst minimising the effects of finite lattice sizes.\n",
    "\n",
    "We shall therefore sample the size of the parent cluster for all of the points from a smaller central cube of side length $L'$ from our lattice of side length $L$, and sum up our results over a large number of runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835c2945",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_size_num_prob_3d(\"data/p_244\", bins=True)\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def rhs(log2_size, tau, omega, a1, a2):\n",
    "  return (-(tau - 2) * log2_size + a1 * 2**(-log2_size * omega) + a2)\n",
    "\n",
    "directory = \"data/p_244_test20\"\n",
    "params = []\n",
    "\n",
    "fig, axs = plt.subplots(10)\n",
    "fig.set_figheight(20)\n",
    "\n",
    "for idx, filename in enumerate(sorted(os.listdir(directory))):\n",
    "  path = os.path.join(directory, filename)\n",
    "\n",
    "  with open(path) as fp:\n",
    "    for i, line in enumerate(fp):\n",
    "      if i == 1:\n",
    "        params = line.split(',')\n",
    "        break\n",
    "\n",
    "  data = np.genfromtxt(path, delimiter=',', skip_header=4)\n",
    "\n",
    "  size_data = [d for i, d in enumerate(data[:, 0]) if data[i, 2] == 0]\n",
    "  number_data = [np.log2(np.sum(data[i:, 1] + data[i:, 2])) for i, d in enumerate(data[:, 1]) if data[i, 2] == 0]\n",
    "  \n",
    "  popt, pcov = curve_fit(rhs, size_data, number_data, p0=(2.19,0.5,0.1,30), bounds=((1, 0, 0, 0), (3, 2, 2, 50)))\n",
    "\n",
    "  print(popt)\n",
    "  print(np.sqrt(np.diag(pcov)))\n",
    "  axs[idx].scatter(size_data, number_data)\n",
    "\n",
    "  pred_num_data = [rhs(s, *popt) for s in size_data]\n",
    "  axs[idx].plot(size_data, pred_num_data)\n",
    "\n",
    "  axs[idx].set_xlabel(\"log size\")\n",
    "  axs[idx].set_title(params[0])\n",
    "  axs[idx].set_ylabel(\"log num_clusters\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Probably better to run over points in central cube and just keep track of what proportion belong to each size?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
